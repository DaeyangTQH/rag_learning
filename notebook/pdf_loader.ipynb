{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chromadb\n",
    "import uuid\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embeddings generation using SentenceTransformer.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the EmbeddingManager with the specified model name.\n",
    "\n",
    "        Args:\n",
    "              model_name (str): Hugging Face model name to use for embedding generation. Defaults to \"all-MiniLM-L6-v2.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model.\"\"\"\n",
    "        try:\n",
    "            print(f'loading embedding model: {self.model_name}')\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f'embedding model loaded successfully. Embedding dimension: {self.get_embedding_dimension()}')\n",
    "        except Exception as e:\n",
    "            print(f\"Erorr loading model {self.model_name}: {e}\")\n",
    "            raise  # Giống với throw exception trong Java\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts.\n",
    "\n",
    "        :param texts: List of text strings to embed\n",
    "        :return: a numpy array of embeddings with shapes (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.model:  # Nếu chưa tồn tại object\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "\n",
    "        print(f\"Generating embedding model for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Embeddings generated successfully. Shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "    def get_embedding_dimension(self) -> int:\n",
    "        \"\"\"Get the dimension of the embedding model\"\"\"\n",
    "\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "\n",
    "        return self.model.get_sentence_embedding_dimension()\n",
    "\n",
    "\n",
    "# Initialize the embedding manager\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ],
   "id": "352218a11f08b01f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### Vector Store Database\n",
    "class VectorStore:\n",
    "    \"\"\"Manages documents embeddings in a ChromaDB vector store\"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "\n",
    "        :param collection_name: Name of the ChromaDB collection to use.\n",
    "        :param persist_directory: Directory to persist to vector store\n",
    "        \"\"\"\n",
    "\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize the ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent chromadb client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            # Get or create a collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                # Giống với table trong db - CREATE TABLE IF NOT EXISTS\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF documents embeddings for RAG\"}\n",
    "            )\n",
    "\n",
    "            print(f\"Vector store initialized successfully. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing ChromaDB client: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add lists of langchain documents and embeddings to the vector store\n",
    "\n",
    "        :param documents: List of langchain documents\n",
    "        :param embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents and embeddings must be equal\")\n",
    "\n",
    "        # Prepare data for db\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(\n",
    "                zip(documents, embeddings)):  # zip để đóng các cặp doc-emb tươgn ứng lại với nhau\n",
    "            #Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            #Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            #Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            #Embedding\n",
    "            embeddings_list.append(embedding)\n",
    "\n",
    "        #Add to a collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Documents added to collection: {self.collection_name}\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to collection: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "vector_store = VectorStore()\n",
    "vector_store"
   ],
   "id": "f02019d278a28990"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
